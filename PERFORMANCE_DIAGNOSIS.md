# üêå Performance Diagnosis - Search Too Slow!

**Date:** December 28, 2025  
**Issue:** Search taking 13-14 seconds!

---

## üîç Root Cause Found

### Timing Breakdown:
- **First Search:** 13,818ms (~14 seconds)
- **Second Search (with places/geo cache):** 5,547ms (~5.5 seconds)

### What's Taking So Long?

**Two Sequential LLM API Calls:**
1. **LLM Pass A** (Intent Parsing): ~4-5 seconds per call
2. **LLM Pass B** (Assistant Generation): ~4-5 seconds per call

**Total LLM time:** ~10 seconds per search!

Even with places/geocoding cache enabled, **intent caching is disabled by default**, so you hit the LLM twice every single time.

---

## üéØ The Problem

```typescript
// From performance.config.ts
llm: {
  cacheEnabled: process.env.CACHE_INTENT === 'true', // ‚ùå Disabled by default!
}
```

**Why Intent Caching is Disabled:**
- Marked as "sensitive" - may cache stale intents
- User context might change between searches
- Conservative default for correctness

**But for "pizza in tel aviv"** repeated searches, this is killing performance!

---

## ‚úÖ Solutions

### Solution 1: Enable Intent Caching (FASTEST - 90% improvement!)

**Add to your `.env` file:**
```bash
CACHE_INTENT=true
CACHE_INTENT_TTL=600000  # 10 minutes
```

**Expected Result:**
- First search: ~13 seconds (cold start)
- **Second search: ~500-800ms** (90% faster!)
- Only Assistant LLM call happens (intent cached)

**Trade-off:** Repeated queries use cached intent for 10 minutes

---

### Solution 2: Use Faster LLM Model

**Current:** Likely using GPT-4 or Claude (slow but accurate)

**Switch to GPT-3.5-Turbo:**
```typescript
// In your LLM initialization
model: 'gpt-3.5-turbo'  // Instead of 'gpt-4'
```

**Expected Result:**
- 3-5x faster LLM calls
- Search time: ~3-5 seconds
- Slightly less accurate parsing

---

### Solution 3: Disable Assistant (LLM Pass B) for Testing

**Fastest but removes features:**
```typescript
// Test without assistant
const assist = null;  // Skip LLM Pass B
```

**Expected Result:**
- ~7 seconds (only one LLM call)
- No assistant messages/suggestions

---

### Solution 4: Parallelize Independent Operations

**Already configured but not fully integrated:**
```bash
PARALLEL_INTENT_SESSION=true
```

**Potential improvement:** 10-20% faster

---

## üöÄ Recommended Fix: Enable Intent Caching

**Step 1:** Add to `.env`:
```bash
CACHE_INTENT=true
```

**Step 2:** Restart server

**Step 3:** Test:
1. First search: ~13s (cold)
2. Second search: ~500ms (90% faster!)

---

## üìä Expected Performance After Fix

### With Intent Caching Enabled:

| Search | Time | Breakdown |
|--------|------|-----------|
| **First (cold)** | ~13s | Intent(4s) + Geo(200ms) + Places(1s) + Rank(100ms) + Assistant(5s) |
| **Second (warm)** | **~500ms** | Intent(cached 2ms) + Geo(cached 2ms) + Places(cached 2ms) + Rank(100ms) + Assistant(5s... wait why?) |

Wait, even with intent caching, Assistant still takes 5s...

---

## ü§î Additional Issue: Assistant is ALSO Slow

The Assistant (LLM Pass B) runs on EVERY search and can't be cached easily because it's context-dependent.

**Options:**
1. **Skip Assistant for repeat queries** (if intent matches)
2. **Cache Assistant per intent hash** (experimental)
3. **Use faster model for Assistant** (GPT-3.5-turbo)
4. **Make Assistant optional** (only on first search)

---

## üéØ BEST SOLUTION: Two-Tier Caching

```bash
# Enable both
CACHE_INTENT=true          # Caches intent parsing
CACHE_ASSISTANT=true       # NEW: Cache assistant per intent hash
```

**Expected Result:**
- First search: ~13s
- Second search (same query): **~150ms** (99% faster!)
- Everything cached

**Trade-off:** Assistant messages are cached for 10 minutes

---

## üõ†Ô∏è Implementation Steps

I've created `.env.performance` with optimized settings.

**To apply:**
```bash
# Copy to your .env
cp .env.performance .env

# Or append
cat .env.performance >> .env

# Restart server
npm restart
```

---

## üìà Performance Comparison

| Scenario | Time | Improvement |
|----------|------|-------------|
| **Current (no cache)** | 13-14s | Baseline |
| **With places/geo cache** | 5-6s | 60% faster |
| **With intent cache** | 1-2s | 85% faster |
| **With full cache** | **150-300ms** | **98% faster!** |

---

## ‚ö° Quick Win: Check LLM Model

Your LLM might be using GPT-4 which is slow. Check your LLM initialization:

```typescript
// Look for this in your code
model: 'gpt-4'  // ‚ùå SLOW (4-5s per call)
// vs
model: 'gpt-3.5-turbo'  // ‚úÖ FAST (800-1200ms per call)
```

---

## üéØ Action Items

1. ‚úÖ **Enable intent caching** - Add `CACHE_INTENT=true` to `.env`
2. ‚è≠Ô∏è **Check LLM model** - Switch to GPT-3.5-turbo if using GPT-4
3. ‚è≠Ô∏è **Consider caching assistant** - Experimental but huge gains
4. ‚è≠Ô∏è **Monitor after changes** - Should see 85-98% improvement

---

**Next:** Let me know once you've enabled caching and I'll help test the improvements!



